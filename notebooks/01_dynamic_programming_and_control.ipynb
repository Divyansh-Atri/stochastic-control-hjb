{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 1: Dynamic Programming and Stochastic Control\n",
                "\n",
                "**Author:** Divyansh Atri\n",
                "\n",
                "## Overview\n",
                "\n",
                "This notebook introduces the fundamental concepts of stochastic optimal control:\n",
                "- Controlled stochastic differential equations (SDEs)\n",
                "- Cost functionals and value functions\n",
                "- The dynamic programming principle\n",
                "- Optimal feedback control policies\n",
                "\n",
                "These concepts form the foundation for deriving and solving the Hamilton-Jacobi-Bellman (HJB) equation in subsequent notebooks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.integrate import odeint\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "# Set plotting style\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "plt.rcParams['font.size'] = 11\n",
                "\n",
                "print(\"Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Stochastic Differential Equations\n",
                "\n",
                "### 1.1 Definition\n",
                "\n",
                "A **controlled stochastic differential equation (SDE)** describes the evolution of a state process $X_t \\in \\mathbb{R}^d$ under the influence of:\n",
                "1. **Deterministic drift** $b(X_t, u_t)$ - controlled dynamics\n",
                "2. **Stochastic diffusion** $\\sigma(X_t)$ - random fluctuations\n",
                "\n",
                "Mathematically:\n",
                "\n",
                "$$\n",
                "dX_t = b(X_t, u_t) \\, dt + \\sigma(X_t) \\, dW_t, \\quad X_0 = x_0\n",
                "$$\n",
                "\n",
                "where:\n",
                "- $X_t$ is the state at time $t$\n",
                "- $u_t$ is the control input at time $t$\n",
                "- $W_t$ is a standard Brownian motion (Wiener process)\n",
                "- $b: \\mathbb{R}^d \\times \\mathbb{R}^m \\to \\mathbb{R}^d$ is the drift coefficient\n",
                "- $\\sigma: \\mathbb{R}^d \\to \\mathbb{R}^{d \\times d}$ is the diffusion coefficient\n",
                "\n",
                "### 1.2 Interpretation\n",
                "\n",
                "The SDE can be understood as:\n",
                "- **Drift term** $b(X_t, u_t) dt$: Deterministic evolution we can influence via control $u_t$\n",
                "- **Diffusion term** $\\sigma(X_t) dW_t$: Random perturbations we cannot control\n",
                "\n",
                "The control $u_t$ allows us to steer the system, but we cannot eliminate the randomness entirely."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.3 Example: Brownian Motion with Drift\n",
                "\n",
                "Consider the simplest case: 1D Brownian motion with constant drift and diffusion:\n",
                "\n",
                "$$\n",
                "dX_t = \\mu \\, dt + \\sigma \\, dW_t\n",
                "$$\n",
                "\n",
                "This has **no control** yet. Let's simulate it to build intuition."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simulate_brownian_motion(mu, sigma, T, dt, n_paths=5, seed=42):\n",
                "    \"\"\"\n",
                "    Simulate Brownian motion with drift using Euler-Maruyama\n",
                "    \n",
                "    dX_t = μ dt + σ dW_t\n",
                "    \"\"\"\n",
                "    np.random.seed(seed)\n",
                "    \n",
                "    t = np.arange(0, T + dt, dt)\n",
                "    n_steps = len(t)\n",
                "    \n",
                "    X = np.zeros((n_paths, n_steps))\n",
                "    \n",
                "    for i in range(n_paths):\n",
                "        dW = np.sqrt(dt) * np.random.randn(n_steps - 1)\n",
                "        for j in range(n_steps - 1):\n",
                "            X[i, j+1] = X[i, j] + mu * dt + sigma * dW[j]\n",
                "    \n",
                "    return t, X\n",
                "\n",
                "# Simulate\n",
                "mu, sigma = 0.1, 0.5\n",
                "T, dt = 5.0, 0.01\n",
                "t, X = simulate_brownian_motion(mu, sigma, T, dt, n_paths=10)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(12, 5))\n",
                "for i in range(X.shape[0]):\n",
                "    plt.plot(t, X[i, :], alpha=0.6, linewidth=1.5)\n",
                "plt.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
                "plt.xlabel('Time $t$', fontsize=12)\n",
                "plt.ylabel('State $X_t$', fontsize=12)\n",
                "plt.title(f'Brownian Motion with Drift: $dX_t = {mu} dt + {sigma} dW_t$', fontsize=14)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Simulated {X.shape[0]} paths over time [0, {T}]\")\n",
                "print(f\"Mean at T={T}: {np.mean(X[:, -1]):.3f} (theoretical: {mu*T:.3f})\")\n",
                "print(f\"Std at T={T}: {np.std(X[:, -1]):.3f} (theoretical: {sigma*np.sqrt(T):.3f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observation:** The paths exhibit random fluctuations around the deterministic trend $\\mu t$. The spread increases with time due to accumulation of randomness."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Controlled SDEs and Admissible Controls\n",
                "\n",
                "### 2.1 Control Processes\n",
                "\n",
                "A **control process** $u = (u_t)_{t \\geq 0}$ is a stochastic process that influences the drift of the SDE.\n",
                "\n",
                "**Admissibility:** A control $u$ is admissible if:\n",
                "1. **Adapted:** $u_t$ depends only on information available up to time $t$ (no future peeking)\n",
                "2. **Integrability:** $\\mathbb{E}\\left[\\int_0^T |u_t|^2 dt\\right] < \\infty$\n",
                "\n",
                "We denote the set of admissible controls by $\\mathcal{U}_{ad}$.\n",
                "\n",
                "### 2.2 Feedback Controls\n",
                "\n",
                "A particularly important class is **Markov feedback controls**:\n",
                "\n",
                "$$\n",
                "u_t = \\alpha(t, X_t)\n",
                "$$\n",
                "\n",
                "where $\\alpha: [0,T] \\times \\mathbb{R}^d \\to \\mathbb{R}^m$ is a deterministic function.\n",
                "\n",
                "**Why feedback?** \n",
                "- The control depends only on current state $X_t$, not the entire history\n",
                "- Easier to implement in practice\n",
                "- Optimal controls often have this form (verification theorem)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 Example: Controlled Brownian Motion\n",
                "\n",
                "Consider:\n",
                "\n",
                "$$\n",
                "dX_t = u_t \\, dt + \\sigma \\, dW_t\n",
                "$$\n",
                "\n",
                "The drift is **directly controlled**. Let's compare different control strategies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simulate_controlled_bm(u_func, sigma, T, dt, x0=0, seed=42):\n",
                "    \"\"\"\n",
                "    Simulate controlled Brownian motion: dX_t = u_t dt + σ dW_t\n",
                "    \n",
                "    Args:\n",
                "        u_func: Control function u(t, x)\n",
                "    \"\"\"\n",
                "    np.random.seed(seed)\n",
                "    \n",
                "    t = np.arange(0, T + dt, dt)\n",
                "    n_steps = len(t)\n",
                "    \n",
                "    X = np.zeros(n_steps)\n",
                "    u = np.zeros(n_steps)\n",
                "    X[0] = x0\n",
                "    \n",
                "    dW = np.sqrt(dt) * np.random.randn(n_steps - 1)\n",
                "    \n",
                "    for i in range(n_steps - 1):\n",
                "        u[i] = u_func(t[i], X[i])\n",
                "        X[i+1] = X[i] + u[i] * dt + sigma * dW[i]\n",
                "    \n",
                "    u[-1] = u_func(t[-1], X[-1])\n",
                "    \n",
                "    return t, X, u\n",
                "\n",
                "# Define different control strategies\n",
                "sigma = 0.5\n",
                "T, dt = 5.0, 0.01\n",
                "\n",
                "# Strategy 1: Zero control (do nothing)\n",
                "u_zero = lambda t, x: 0.0\n",
                "\n",
                "# Strategy 2: Constant positive drift\n",
                "u_const = lambda t, x: 0.5\n",
                "\n",
                "# Strategy 3: Proportional feedback (stabilizing)\n",
                "u_feedback = lambda t, x: -0.5 * x\n",
                "\n",
                "# Simulate\n",
                "t, X_zero, u_z = simulate_controlled_bm(u_zero, sigma, T, dt)\n",
                "t, X_const, u_c = simulate_controlled_bm(u_const, sigma, T, dt)\n",
                "t, X_feedback, u_f = simulate_controlled_bm(u_feedback, sigma, T, dt)\n",
                "\n",
                "# Plot trajectories\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "axes[0].plot(t, X_zero, label='Zero control', linewidth=2)\n",
                "axes[0].plot(t, X_const, label='Constant drift', linewidth=2)\n",
                "axes[0].plot(t, X_feedback, label='Feedback control', linewidth=2)\n",
                "axes[0].axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
                "axes[0].set_xlabel('Time $t$', fontsize=12)\n",
                "axes[0].set_ylabel('State $X_t$', fontsize=12)\n",
                "axes[0].set_title('State Trajectories', fontsize=13)\n",
                "axes[0].legend(fontsize=11)\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "axes[1].plot(t, u_z, label='Zero control', linewidth=2)\n",
                "axes[1].plot(t, u_c, label='Constant drift', linewidth=2)\n",
                "axes[1].plot(t, u_f, label='Feedback control', linewidth=2)\n",
                "axes[1].axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
                "axes[1].set_xlabel('Time $t$', fontsize=12)\n",
                "axes[1].set_ylabel('Control $u_t$', fontsize=12)\n",
                "axes[1].set_title('Control Signals', fontsize=13)\n",
                "axes[1].legend(fontsize=11)\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Control Strategies Comparison:\")\n",
                "print(f\"Zero control: Final state = {X_zero[-1]:.3f}\")\n",
                "print(f\"Constant drift: Final state = {X_const[-1]:.3f}\")\n",
                "print(f\"Feedback control: Final state = {X_feedback[-1]:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observation:** \n",
                "- Zero control: Pure random walk\n",
                "- Constant drift: Systematic upward trend\n",
                "- Feedback control: Stabilizes around origin (mean-reverting behavior)\n",
                "\n",
                "The feedback control $u_t = -0.5 X_t$ creates a restoring force toward zero."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Cost Functionals\n",
                "\n",
                "### 3.1 Definition\n",
                "\n",
                "To formulate an optimal control problem, we need a **cost functional** that quantifies performance.\n",
                "\n",
                "The standard form is:\n",
                "\n",
                "$$\n",
                "J(u) = \\mathbb{E}\\left[ \\int_0^T L(X_t, u_t) \\, dt + g(X_T) \\right]\n",
                "$$\n",
                "\n",
                "where:\n",
                "- $L(x, u)$ is the **running cost** (instantaneous cost at each time)\n",
                "- $g(x)$ is the **terminal cost** (cost at final time $T$)\n",
                "- The expectation is over all randomness (Brownian motion)\n",
                "\n",
                "### 3.2 Interpretation\n",
                "\n",
                "- **Running cost** $L(x, u)$: Penalizes undesirable states and expensive controls\n",
                "  - Example: $L(x, u) = \\frac{1}{2}(q x^2 + r u^2)$ penalizes large states and large controls\n",
                "  \n",
                "- **Terminal cost** $g(x)$: Penalizes ending far from target\n",
                "  - Example: $g(x) = \\frac{1}{2} q_T (x - x_{target})^2$\n",
                "\n",
                "### 3.3 Quadratic Cost (LQR)\n",
                "\n",
                "The most important example is the **Linear-Quadratic Regulator (LQR)**:\n",
                "\n",
                "$$\n",
                "L(x, u) = \\frac{1}{2}(q x^2 + r u^2), \\quad g(x) = \\frac{1}{2} q_T x^2\n",
                "$$\n",
                "\n",
                "where $q, r, q_T > 0$ are weights.\n",
                "\n",
                "**Interpretation:**\n",
                "- $q$: Penalty on state deviation (want $x$ small)\n",
                "- $r$: Penalty on control effort (want $u$ small)\n",
                "- $q_T$: Terminal penalty (want $x_T$ small)\n",
                "\n",
                "**Tradeoff:** Large $r$ means expensive control → accept larger state deviations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_cost(t, X, u, q, r, q_T, dt):\n",
                "    \"\"\"\n",
                "    Compute quadratic cost: J = ∫(1/2)(qx² + ru²)dt + (1/2)q_T x_T²\n",
                "    \"\"\"\n",
                "    # Running cost (trapezoidal rule)\n",
                "    running_cost = 0.5 * (q * X**2 + r * u**2)\n",
                "    integral = np.trapz(running_cost, dx=dt)\n",
                "    \n",
                "    # Terminal cost\n",
                "    terminal_cost = 0.5 * q_T * X[-1]**2\n",
                "    \n",
                "    return integral + terminal_cost\n",
                "\n",
                "# Compute costs for different strategies\n",
                "q, r, q_T = 1.0, 1.0, 10.0\n",
                "\n",
                "cost_zero = compute_cost(t, X_zero, u_z, q, r, q_T, dt)\n",
                "cost_const = compute_cost(t, X_const, u_c, q, r, q_T, dt)\n",
                "cost_feedback = compute_cost(t, X_feedback, u_f, q, r, q_T, dt)\n",
                "\n",
                "print(\"Cost Comparison (q=1, r=1, q_T=10):\")\n",
                "print(f\"Zero control:     J = {cost_zero:.4f}\")\n",
                "print(f\"Constant drift:   J = {cost_const:.4f}\")\n",
                "print(f\"Feedback control: J = {cost_feedback:.4f}\")\n",
                "print(f\"\\nBest strategy: {'Feedback' if cost_feedback < min(cost_zero, cost_const) else 'Other'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observation:** The feedback control achieves lower cost by balancing state deviation and control effort."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. The Optimal Control Problem\n",
                "\n",
                "### 4.1 Problem Statement\n",
                "\n",
                "Given:\n",
                "- Controlled SDE: $dX_t = b(X_t, u_t) dt + \\sigma(X_t) dW_t$\n",
                "- Cost functional: $J(u) = \\mathbb{E}\\left[\\int_0^T L(X_t, u_t) dt + g(X_T)\\right]$\n",
                "\n",
                "**Find:** The optimal control $u^* \\in \\mathcal{U}_{ad}$ that minimizes the cost:\n",
                "\n",
                "$$\n",
                "u^* = \\arg\\min_{u \\in \\mathcal{U}_{ad}} J(u)\n",
                "$$\n",
                "\n",
                "### 4.2 Value Function\n",
                "\n",
                "The **value function** is the minimum achievable cost starting from state $x$ at time $t$:\n",
                "\n",
                "$$\n",
                "V(t, x) = \\inf_{u \\in \\mathcal{U}_{ad}} \\mathbb{E}_{t,x}\\left[ \\int_t^T L(X_s, u_s) ds + g(X_T) \\right]\n",
                "$$\n",
                "\n",
                "where $\\mathbb{E}_{t,x}$ denotes expectation conditioned on $X_t = x$.\n",
                "\n",
                "**Key Properties:**\n",
                "1. $V(T, x) = g(x)$ (terminal condition)\n",
                "2. $V(0, x_0)$ is the optimal cost starting from $x_0$\n",
                "3. The optimal control can be extracted from $V$ (feedback form)\n",
                "\n",
                "### 4.3 Intuition\n",
                "\n",
                "Think of $V(t, x)$ as:\n",
                "- **\"Cost-to-go\"**: How much it will cost (on average) to run optimally from $(t, x)$ to the end\n",
                "- **Depends on both time and state**: Earlier times and worse states typically have higher cost-to-go\n",
                "- **Decreases with time**: As we approach terminal time $T$, only terminal cost $g(X_T)$ remains"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. The Dynamic Programming Principle\n",
                "\n",
                "### 5.1 Statement\n",
                "\n",
                "The **Dynamic Programming Principle (DPP)** is the cornerstone of stochastic control theory.\n",
                "\n",
                "**Informal Statement:** An optimal policy has the property that, no matter what the initial state and decision, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\n",
                "\n",
                "**Formal Statement:** For any $0 \\leq t \\leq s \\leq T$:\n",
                "\n",
                "$$\n",
                "V(t, x) = \\inf_{u \\in \\mathcal{U}_{ad}} \\mathbb{E}_{t,x}\\left[ \\int_t^s L(X_r, u_r) dr + V(s, X_s) \\right]\n",
                "$$\n",
                "\n",
                "### 5.2 Interpretation\n",
                "\n",
                "The DPP says:\n",
                "1. **Break the problem into pieces**: Cost from $t$ to $T$ = Cost from $t$ to $s$ + Cost from $s$ to $T$\n",
                "2. **Optimize both pieces**: The control must be optimal on both $[t, s]$ and $[s, T]$\n",
                "3. **Value function appears recursively**: $V(s, X_s)$ is the cost-to-go from intermediate state\n",
                "\n",
                "This is analogous to Bellman's principle in discrete-time dynamic programming.\n",
                "\n",
                "### 5.3 Infinitesimal Version\n",
                "\n",
                "Taking $s = t + h$ for small $h > 0$:\n",
                "\n",
                "$$\n",
                "V(t, x) = \\inf_{u} \\mathbb{E}_{t,x}\\left[ \\int_t^{t+h} L(X_r, u_r) dr + V(t+h, X_{t+h}) \\right]\n",
                "$$\n",
                "\n",
                "Expanding and taking $h \\to 0$ leads to the **Hamilton-Jacobi-Bellman equation** (next notebook).\n",
                "\n",
                "### 5.4 Why DPP Matters\n",
                "\n",
                "1. **Reduces infinite-dimensional problem**: Instead of optimizing over all control functions $u: [0,T] \\to \\mathbb{R}$, we solve a PDE for $V(t,x)$\n",
                "2. **Enables feedback synthesis**: Optimal control is $u^*(t,x) = \\arg\\min_u \\{\\text{Hamiltonian}\\}$\n",
                "3. **Provides verification**: If we guess a solution $V$, we can verify optimality via DPP"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Summary and Next Steps\n",
                "\n",
                "### Key Concepts Introduced\n",
                "\n",
                "1. **Controlled SDEs**: $dX_t = b(X_t, u_t) dt + \\sigma(X_t) dW_t$\n",
                "   - Drift $b$ is influenced by control $u_t$\n",
                "   - Diffusion $\\sigma$ represents uncontrollable randomness\n",
                "\n",
                "2. **Cost Functional**: $J(u) = \\mathbb{E}\\left[\\int_0^T L(X_t, u_t) dt + g(X_T)\\right]$\n",
                "   - Running cost $L$ penalizes states and controls\n",
                "   - Terminal cost $g$ penalizes final state\n",
                "\n",
                "3. **Value Function**: $V(t,x) = \\inf_u \\mathbb{E}_{t,x}[J(u)]$\n",
                "   - Minimum achievable cost from $(t,x)$\n",
                "   - Central object of study\n",
                "\n",
                "4. **Dynamic Programming Principle**:\n",
                "   - Recursive structure of optimal control\n",
                "   - Foundation for deriving HJB equation\n",
                "\n",
                "### What We've Learned\n",
                "\n",
                "- Different control strategies lead to different costs\n",
                "- Feedback controls can stabilize stochastic systems\n",
                "- The value function encodes optimal cost-to-go\n",
                "- DPP provides a recursive characterization\n",
                "\n",
                "### Next Notebook\n",
                "\n",
                "In **Notebook 2**, we will:\n",
                "1. Derive the Hamilton-Jacobi-Bellman equation from the DPP\n",
                "2. Understand it as a nonlinear PDE\n",
                "3. Interpret the Hamiltonian and optimality conditions\n",
                "4. Discuss viscosity solutions\n",
                "\n",
                "This will transform the infinite-dimensional optimization problem into a PDE that we can solve numerically."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References\n",
                "\n",
                "1. Fleming, W. H., & Soner, H. M. (2006). *Controlled Markov Processes and Viscosity Solutions*. Springer.\n",
                "2. Yong, J., & Zhou, X. Y. (1999). *Stochastic Controls: Hamiltonian Systems and HJB Equations*. Springer.\n",
                "3. Pham, H. (2009). *Continuous-time Stochastic Control and Optimization with Financial Applications*. Springer."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}